# -*- coding: utf-8 -*-
"""Malware_ML.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IvwlKhW-HzrN47S8fQC5_aLGDcDF9DT0
"""

# Importing Libraries
import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from matplotlib.pyplot import figure

pd.set_option('display.max_columns', None)

# Mounting the notebook to the drive to load the data
from google.colab import files
uploaded = files.upload()

# Loading data
df = pd.read_csv('project data.csv')
df

df.Result.value_counts()

df.shape

pd.set_option('display.max_rows', None)
print(df.dtypes)
pd.reset_option('display.max_rows')

df.describe()

df.columns

# Setting the display option to show all rows and columns
pd.set_option('display.max_rows', None)
pd.set_option('display.max_columns', None)

missing_values = df.isnull().any()

# Printing the missing values for all columns
print(missing_values)

# Resetting the display options to their defaults
pd.reset_option('display.max_rows')
pd.reset_option('display.max_columns')

#Plotting a correlation Matrix
fig, ax = plt.subplots(figsize=(150,150))
sns.heatmap(df.corr(),annot=True,cmap='Greens')

#Dropping Highly correlated features
def drop_highly_correlated_features(df, threshold=0.9):
    # Creating a correlation matrix
    corr_matrix = df.corr().abs()

    # Creating a mask to identify highly correlated features
    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(np.bool))

    # Finding features with correlation above the threshold
    to_drop = [column for column in upper.columns if any(upper[column] > threshold)]

    # Droping the highly correlated features
    df_dropped = df.drop(columns=to_drop)

    return df_dropped

# Loading the dataset into a Pandas DataFrame
new_data = pd.read_csv('project data.csv')

# Setting a correlation threshold (e.g., 0.9)
correlation_threshold = 0.9

# Dropping highly correlated features
data_no_corr = drop_highly_correlated_features(new_data, correlation_threshold)
df1 = data_no_corr

df1.shape

from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error

"""## Data Mining Models"""

X = df1.drop(['Result'], 1)
Y = df1.iloc[:, -1].values

from sklearn.model_selection import train_test_split
X_training, X_testing, Y_training, Y_testing = train_test_split(X, Y, test_size = 0.3, random_state = 0)

X_training.describe()

X_testing.describe()

"""### Logistic Regression"""

from sklearn.linear_model import LogisticRegression
from sklearn import metrics

logreg = LogisticRegression()
logreg.fit(X_training, Y_training)

from sklearn import metrics
from sklearn.metrics import r2_score
from sklearn.metrics import f1_score

from sklearn.metrics import confusion_matrix, accuracy_score

Y_Logreg = logreg.predict(X_testing)
Conf_Logreg = confusion_matrix(Y_testing, Y_Logreg)
Accuracy_Logreg = accuracy_score(Y_testing, Y_Logreg)

Conf_Logreg

Accuracy_Logreg

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve

ROC_Logreg = roc_auc_score(Y_testing, Y_Logreg)
fpr, tpr, thresholds = roc_curve(Y_testing, logreg.predict_proba(X_testing)[:,1])

plt.figure(figsize=(10,10))
plt.plot(fpr, tpr)

plt.plot([0, 1], [0, 1],'r--')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])

plt.legend(loc="upper right")

plt.title('Receiver Operating Characteristic for Logistic Regression')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')

plt.savefig('Log_ROC')

plt.show()

# https://stackoverflow.com/questions/4700614/how-to-put-the-legend-outside-the-plot

""" Calculating Area under the curve"""

ROC_Logreg

MAE_Logreg = metrics.mean_absolute_error(Y_testing, Y_Logreg)
RMSE_Logreg = np.sqrt(metrics.mean_squared_error(Y_testing, Y_Logreg))
Sensitivity_Logreg = Conf_Logreg[0,0]/(Conf_Logreg[0,0] + Conf_Logreg[0,1])
Specificity_Logreg = Conf_Logreg[1,1]/(Conf_Logreg[1,0] + Conf_Logreg[1,1])
R2_Logreg = r2_score(Y_testing, Y_Logreg)
F1_Logreg = f1_score(Y_testing, Y_Logreg)

PPV_Logreg = Conf_Logreg[0,0]/(Conf_Logreg[0,0] + Conf_Logreg[1,0])
print('PPV_Logreg : ', PPV_Logreg)

print('accuracy score: ', Accuracy_Logreg)
print('Error:', 1 - Accuracy_Logreg)
print('Sensitivity : ', Sensitivity_Logreg )
print('Specificity : ', Specificity_Logreg)
print('F1 Score: ', F1_Logreg)

"""### KNN"""

KNN_Y = Y
X_training, X_testing, Y_training, Y_testing = train_test_split(X, KNN_Y, test_size = 0.3, random_state = 0)
# X_training.iloc[:, :74]= sc.fit_transform(X_training.iloc[:, :74])
# X_testing.iloc[:, :74] = sc.transform(X_testing.iloc[:, :74])

from sklearn.metrics import accuracy_score
from sklearn.neighbors import KNeighborsClassifier

KNN_values = []

for k in range(1, 25):
  KNN_calc = KNeighborsClassifier(n_neighbors=k).fit(X_training, Y_training)
  KNN_values.append({
  'k': k,
  'accuracy': accuracy_score(Y_testing, KNN_calc.predict(X_testing))
  })

result= pd.DataFrame(KNN_values)
print(KNN_values)

"""Thus, we can spot the best result for k = 3

"""

KNN_values = KNeighborsClassifier(n_neighbors=25).fit(X_training, Y_training)
KNN_Y = KNN_values.predict(X_testing)
Conf_KNN = confusion_matrix(Y_testing, KNN_Y)
Accuracy_KNN = accuracy_score(Y_testing,KNN_Y)

Conf_KNN

Accuracy_KNN

ROC_KNN = roc_auc_score(Y_testing, KNN_Y)
fpr, tpr, thresholds = roc_curve(Y_testing, KNN_values.predict_proba(X_testing)[:,1])

plt.figure(figsize=(10,10))
plt.plot(fpr, tpr)

plt.plot([0, 1], [0, 1],'r--')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])

plt.legend(loc="upper right")

plt.title('Receiver Operating Characteristic for KNN Model')

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')

plt.savefig('ROC_KNN')

plt.show()

"""Area under the curve"""

ROC_KNN

MAE_KNN = metrics.mean_absolute_error(Y_testing, KNN_Y)
RMSE_KNN = np.sqrt(metrics.mean_squared_error(Y_testing, KNN_Y))
Sensitivity_KNN = Conf_KNN[0,0]/(Conf_KNN[0,0] + Conf_KNN[0,1])
Specificity_KNN = Conf_KNN[1,1]/(Conf_KNN[1,0] + Conf_KNN[1,1])
#R2_KNN = r2_score(Y_testing, KNN_Y)
F1_KNN = f1_score(Y_testing, KNN_Y)
PPV_KNN = PPV_KNN = Conf_KNN[0,0]/(Conf_KNN[0,0] + Conf_KNN[1,0])

print('accuracy score: ', Accuracy_KNN)
print('Error:', 1 - Accuracy_KNN)
print('Sensitivity : ', Sensitivity_KNN )
print('Specificity : ', Specificity_KNN)
#print('R^2: ', NB_R2)
print('PPV_KNN : ', PPV_KNN)
print('F1 Score: ', F1_KNN)

"""### DECISION TREES"""

from sklearn.tree import DecisionTreeClassifier
tree_depth = [1,2,3,4,5,6,7,8,9,10]
impurity = ['gini', 'entropy']

gini_impurity = []
entropy_impurity = []

for y in impurity:
  for x in tree_depth:
      dtc = DecisionTreeClassifier(max_depth=x)
      dtc.fit(X_training,Y_training)
      pred_dtc = dtc.predict(X_testing)
      if (y== 'gini'):
        gini_impurity.append(accuracy_score(Y_testing, pred_dtc))
      else:
        entropy_impurity.append(accuracy_score(Y_testing,pred_dtc))

x = np.arange(len(tree_depth)) + 1
# Creating domain for plot

plt.plot(x, gini_impurity, label='Ginni')
plt.plot(x, entropy_impurity, label='Entropy')

plt.xlabel('Maximum Depth')
plt.ylabel('Accuracy')
plt.legend()
plt.show()

Decision_tree = DecisionTreeClassifier(criterion="entropy", max_depth=6)
Decision_tree = Decision_tree.fit(X_training,Y_training)
Y_Dectree_pred = Decision_tree.predict(X_testing)

conf_dectree = confusion_matrix(Y_testing, Y_Dectree_pred)
acc_dectree = accuracy_score(Y_testing,Y_Dectree_pred)
print(conf_dectree)
print('accuracy score: ', acc_dectree)

from sklearn.metrics import roc_auc_score
from sklearn.metrics import roc_curve
Dec_Tree_roc_auc = roc_auc_score(Y_testing, Y_Dectree_pred)
fpr, tpr, thresholds = roc_curve(Y_testing, Decision_tree.predict_proba(X_testing)[:,1])

plt.figure(figsize=(10,10))

plt.plot(fpr, tpr)
plt.plot([0, 1], [0, 1],'r--')

plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])

plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')

plt.legend(loc="upper right")

plt.title('Receiver Operating Characteristic for Decision Trees')

plt.show()

Dec_Tree_roc_auc

# Performance Evaluation and Interpretation ---- Decision Tree
from sklearn import metrics
from sklearn.metrics import r2_score
from sklearn.metrics import f1_score

DecTree_R2 = r2_score(Y_testing, Y_Dectree_pred)
DecTree_F1 = f1_score(Y_testing, Y_Dectree_pred)
DecTree_MAE = metrics.mean_absolute_error(Y_testing, Y_Dectree_pred)
DecTree_RMSE = np.sqrt(metrics.mean_squared_error(Y_testing, Y_Dectree_pred))
DecTree_sensitivity = conf_dectree[0,0]/(conf_dectree[0,0] + conf_dectree[0,1])
DecTree_specificity = conf_dectree[1,1]/(conf_dectree[1,0]+conf_dectree[1,1])

PPV_DT = conf_dectree[0,0]/(conf_dectree[0,0] + conf_dectree[1,0])
print('PPV_DT : ', PPV_DT)

print('accuracy score: ', acc_dectree)
print('Error', 1-acc_dectree)
print('Sensitivity : ', DecTree_sensitivity )
print('Specificity : ', DecTree_specificity)
#print('R^2: ', DT_R2)
print('F1 Score: ', DecTree_F1)
#print('Mean Absolute Error:', DT_MAE)
#print('Root Mean Squared Error:', DT_RMSE)

"""### Comapring Accuracy Across Models"""

Acc_df = pd.DataFrame({'All_Models': ['Logistic Regression', 'K Nearest Neighbors', 'Decision Trees'],
                           'Accuracy': [Accuracy_Logreg, Accuracy_KNN,acc_dectree]
                           })
sns.set(rc={'figure.figsize':(13,9)})
sns.barplot(x = 'All_Models', y = 'Accuracy', data = Acc_df, palette= "dark:#5A9_r" ).set(title='Comapring Accuracy Across Models')

"""### Comapring F1 Scores Across Models"""

F1_df = pd.DataFrame({'All_Models': ['Logistic Regression', 'K Nearest Neighbors', 'Decision Trees'],
                           'F1': [F1_Logreg, F1_KNN, DecTree_F1]
                           })
sns.set(rc={'figure.figsize':(13,9)})
sns.barplot(x = 'All_Models', y = 'F1', data = F1_df, palette= "dark:#5A9_r" ).set(title='Comapring F1 Across Models')

"""### Comapring ROC AUC Across Models"""

ROC_df = pd.DataFrame({'All_Models': ['Logistic Regression', 'K Nearest Neighbors', 'Decision Trees'],
                           'ROC': [ROC_Logreg, ROC_KNN, Dec_Tree_roc_auc]
                           })
sns.set(rc={'figure.figsize':(13,9)})
sns.barplot(x = 'All_Models', y = 'ROC', data = ROC_df, palette= "dark:#5A9_r" ).set(title='Comapring ROC AUC Across Models')